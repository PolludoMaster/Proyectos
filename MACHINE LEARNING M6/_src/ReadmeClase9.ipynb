{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HenryLogo](https://d31uz8lwfmyn8g.cloudfront.net/Assets/logo-henry-white-lg.png)\n",
    "\n",
    "## Redes Neuronales\n",
    "\n",
    "El modelo de neuronas artificiales inspirado en el comportamiento biológico de las neuronas y en cómo se organizan formando la estructura del cerebro, publicado por McCulloch y Pitts en 1943, se considera el primer trabajo en el campo de la Inteligencia Artificial.\n",
    "\n",
    "En una neurona, podemos reconocer diferentes partes:\n",
    "* El cuerpo central, llamado soma, que contiene el núcleo celular.\n",
    "* Una prolongación del soma, el axón. \n",
    "* Una ramificación terminal, dendritas.\n",
    "* Una zona de conexión entre una neurona y otra, conocida como sinapsis.\n",
    "\n",
    "<img src=\"../_src/assets/redes_neuronales1.jpg\" height=\"400\"><br>\n",
    "\n",
    "Trece años más tarde, en 1956, se acuñaría el propio término “Inteligencia Artificial” por John McCarthy, Marvin Minsky y Claude Shannon en una conferencia en Darthmounth.\n",
    "En 1958 Frank Rosenblatt diseña la primera red neuronal artificial, el Perceptrón.\n",
    "\n",
    "Modelo neuronal con n entradas, que consta de\n",
    "* Un conjunto de entradas x1,…xn\n",
    "* Los pesos sinápticos w1,…wn, correspondientes a cada entrada\n",
    "* Una función de agregación, Σ\n",
    "* Una función de activación, f\n",
    "* Una salida:<br>\n",
    "<img src=\"../_src/assets/redes_neuronales2.jpg\" height=\"100\"><br>\n",
    "<img src=\"../_src/assets/redes_neuronales3.jpg\" height=\"400\"><br>\n",
    "\n",
    "Si por ejemplo, se quisiera determinar si va a llover o no, y sabemos que esto depende esencialmente de la diferencia de temperatura entre el aire cercano a la superficie y el aire en la altura, que debe ser más frío; y por otra parte, también depende de la humedad, con lo que, si se dan ambas variables llueve. \n",
    "\n",
    "Las dos variables son:\n",
    "x₁ : Diferencia de temperatura.\n",
    "x₂ : Humedad\n",
    "\n",
    "Teniendo en cuenta que cada una es binaria, es decir si hay la diferencia de temperatura necesaria o no, y si hay la suficiente humedad, entonces se puede armar la tabla:\n",
    "\n",
    "<img src=\"../_src/assets/redes_neuronales4.jpg\" height=\"200\"><br>\n",
    "\n",
    "Convirtiendo los “Sí” y “No” en “1” y “0” respectivamente nos queda el siguiente esquema:<br>\n",
    "\n",
    "<img src=\"../_src/assets/redes_neuronales5.jpg\" height=\"200\"><br>\n",
    "\n",
    "Se pueden ver las 2 entradas, más un valor de sesgo ó “bias”, con su entrada que siempre vale 1.  \n",
    "Por otra parte, también están los pesos sinápticos, es decir, los ponderadores (w1, w2 y w3).\n",
    "\n",
    "El siguiente paso, es encontrar los valores de w1, w2 y w3 que separen correctamente las clases:\n",
    "\n",
    "<img src=\"../_src/assets/redes_neuronales6.jpg\" height=\"200\"><br>\n",
    "\n",
    "La recta <b>x₂ = -4/5 x₁ + 6/5</b> logra separar correctamente las clases. Notar que cuando x₁ y x₂ tienen el valor 1, la sumatoria es 3, es decir, mayor a 0, ante cualquier otra combinación de valores de x₁ y x₂ el resultado es menor a 0.\n",
    "Entonces, una neurona artificial, en su núcleo, es un problema de <b>Regresión Lineal</b>.\n",
    "\n",
    "Por analogía, podemos ver que esta gráfica, corresponde a la tabla de verdad de las compuertas lógicas <b>AND</b><br>\n",
    "<img src=\"../_src/assets/redes_neuronales7.jpg\" height=\"200\"><br>\n",
    "Así mismo, se podría presentar un problema, cuya solución corresponda con la compuerta lógica <b>OR</b><br>\n",
    "<img src=\"../_src/assets/redes_neuronales8.jpg\" height=\"200\"><br>\n",
    "Pero que pasa con la compuerta lógica <b>XOR</b>, mediante la gráfica, se puede ver que no es posible separar las clases con una sola recta.<br>\n",
    "<img src=\"../_src/assets/redes_neuronales9.jpg\" height=\"200\"><br>\n",
    "Este problema existe desde que se formuló la neurona artificial, y dio origen a las redes neuronales, ya que, con dos neuronas, sí es posible resolverlo:<br>\n",
    "<img src=\"../_src/assets/redes_neuronales10.jpg\" height=\"300\"><br>\n",
    "\n",
    "* Agregar capas a la red, permite obtener <b>conocimiento jerarquizado</b>.\n",
    "* Siguiendo el ejemplo, de si va a llover o no, podemos querer predecir información más compleja, como por ejemplo si podremos aprender más sobre Machine Learning, agregando dos entradas más:\n",
    "  * Voy a leer el material sobre el tema que tengo disponible.\n",
    "  * Me voy a quedar mirando tele.<br>\n",
    "\n",
    "<img src=\"../_src/assets/redes_neuronales11.jpg\" height=\"300\"><br>\n",
    "\n",
    "* Notar que ahora, es una red de 3 neuronas, con 13 w\n",
    "\n",
    "A través del algebra matricial, este proceso se puede representar como el <b>producto escalar</b>:<br>\n",
    "\n",
    "<img src=\"../_src/assets/redes_neuronales12.jpg\" height=\"300\"><br>\n",
    "\n",
    "Sin embargo, lo que implementa la función de agregación, es una función lineal. Lo que hace que una neurona artificial no sea sólo una función lineal es la <b>función de activación</b>, que consiste en, dado un valor umbral, arrojar una salida 0 ó 1:\n",
    "\n",
    "Para nuestro ejemplo, la función de activación puede definirse:\n",
    "\n",
    "<img src=\"../_src/assets/redes_neuronales13.jpg\" height=\"100\"><br>\n",
    "<img src=\"../_src/assets/redes_neuronales14.jpg\" height=\"300\"><br>\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "Las neuronas se agrupan en capas, la primera capa (de entrada) tiene tantas neuronas como variables de entrada tiene el planteo del problema, mientras que la ultima capa tantas neuronas como salidas haya, si es binaria, puede ser sólo una neurona. Las capas que están entre la capa de entrada y la de salida, se llaman capas ocultas.\n",
    "Cada neurona tiene sus propios pesos/parámetros. En aplicaciones comunes suelen ser desde miles a millones de parámetros para toda la red.\n",
    "Esa obtención de conocimiento jerarquizado, es lo que da nombre al <b>Aprendizaje Profundo ó Deep Learning</b>, y requiere de encontrar esos pesos W de manera eficiente, bajo la condición de realizar correctamente una tarea objetivo.<br>\n",
    "<img src=\"../_src/assets/redes_neuronales15.jpg\" height=\"400\"><br>\n",
    "\n",
    "### Aprendizaje de Redes Neuronales\n",
    "\n",
    "Es un problema de Regresión Lineal, por ese motivo buscamos entonces Y = mX + b que mejor ajuste a los datos\n",
    "* m: pendiente\n",
    "* b: ordenada al origen\n",
    "\n",
    "1) Es necesario definir cuál es el mejor ajuste a los datos.\n",
    "2) Se puede probar con distintos valores de m y b, y quedarse con el que mejor ajusta.<br>\n",
    "<img src=\"../_src/assets/redes_neuronales16.jpg\" height=\"300\"><br>\n",
    "<img src=\"../_src/assets/redes_neuronales17.jpg\" height=\"300\"><br>\n",
    "\n",
    "Con un modelo paramétrico (y = mx + b, m y b parámetros), se puede hacer una búsqueda por fuerza bruta para encontrar el m y b.\n",
    "Y guiar la búsqueda con una función de costo.\n",
    "Muy similar a GridSearch, pero en este caso es para buscar los parámetros del modelo, no sus hiperparámetros. Ejemplo de hiperparámetro: grado del polinomio. \n",
    "Esta clase de problemas se conocen como problemas de <b>optimización</b>:<br>\n",
    "<img src=\"../_src/assets/redes_neuronales18.jpg\" height=\"300\"><br>\n",
    "\n",
    "## Descenso por Gradiente\n",
    "\n",
    "Computacionalmente, es muy costosa la búsqueda de esos parámetros ya que crece con las dimensiones. Entonces, ¿cuáles son las opciones?\n",
    "\n",
    "Formas analíticas (fórmulas matemáticas) que calculen exactamente el mínimo de la función de costo y a qué parámetros corresponde ese mínimo. Pocos casos pueden resolverse así.\n",
    "Ejemplo: cuadrados mínimos para la regresión lineal.<br>\n",
    "<img src=\"../_src/assets/redes_neuronales19.jpg\" height=\"100\"><br>\n",
    "\n",
    "Mezclar los dos mundos, es decir, combinar la búsqueda en un espacio de parámetros con una guía que nos diga dónde buscar.\n",
    "<b>Descenso por Gradiente</b> consiste en buscar un mínimo global mediante iteraciones en las cuales se va descendiendo en la función de costo J( θ0, θ1):<br>\n",
    "<img src=\"../_src/assets/redes_neuronales20.jpg\" height=\"300\"><br>\n",
    "\n",
    "Se quiere explorar el mínimo de la función de costo, pero sin hacerlo de forma exhaustiva\n",
    "\n",
    "Pasos\n",
    "1) Calculamos el costo para ciertos valores al azar de los parámetros.\n",
    "2) Repetimos este paso hasta converger al mínimo\n",
    "  1) Nos fijamos la dirección de decrecimiento en ese punto. Técnicamente, <b>derivamos o calculamos el gradiente</b>.\n",
    "  2) Nos movemos según el <b>ratio de aprendizaje</b> (es el tamaño de la flecha)\n",
    "  3) <b>Actualizamos los valores de los parámetros</b>.\n",
    "<img src=\"../_src/assets/redes_neuronales21.jpg\" height=\"300\"><br>\n",
    "\n",
    "* <b>Es necesario definir una función de costo/pérdida</b>. La función de costo depende del problema (clasificación, regresión, etc).\n",
    "* La función de costo es una <b>función de los parámetros de la red</b>.\n",
    "* Los mejores parámetros de la red son aquellos que <b>minimicen la función de costo</b>.\n",
    "* Como explorar todo ese espacio de parámetros exhaustivamente (similar a Grid Search) es imposible, se utiliza una técnica que lo haga eficientemente. Esa técnica es Descenso por Gradiente.\n",
    "* <b>Reescalar garantiza la convergencia del algoritmo de Descenso por Gradiente</b> en el entrenamiento.\n",
    "\n",
    "Enlaces recomendados:\n",
    "* [Regresión Lineal y Mínimos cuadrados ordinarios] (https://www.youtube.com/watch?v=w2RJ1D6kz-o)\n",
    "* [¿Qué es el descenso por Gradiente?] (https://www.youtube.com/watch?v=A6FiCDoz8_4)\n",
    "* [¿Qué es el descenso por Gradiente (2)?] (https://www.youtube.com/watch?v=-_A_AAxqzCg)\n",
    "* http://www.benfrederickson.com/numerical-optimization/\n",
    "* https://www.desmos.com/calculator/l0puzw0zvm\n",
    "\n",
    "## Fordward Propagation\n",
    "\n",
    "* Cada valor w, afecta a la siguiente capa, y por ende, a todo el resto de la red neuronal, por tal motivo tiene parte de la responsabilidad en la función de Costo final.\n",
    "* Esa función de Costo o de pérdida, determina cuán lejos está el resultado de la red contra el resultado esperado.\n",
    "* El Descenso por gradiente calcula la derivada/gradiente del costo y con eso actualiza los parámetros. Este proceso lo va a hacer muchas veces hasta llegar al mínimo.\n",
    "* En cada una de esas iteraciones, tiene que calcular el costo. <b>El costo depende de las instancias de entrenamiento y de los parámetros que tengamos hasta ese momento</b>.\n",
    "* Calcular el costo con las instancias de entrenamiento es lo que se conoce como <b>Forward propagation</b>.</b>.\n",
    "<img src=\"../_src/assets/redes_neuronales22.jpg\" height=\"300\"><br>\n",
    "\n",
    "## Back Propagation\n",
    "\n",
    "Con el costo calculado, se actualizan los valores de los parámetros ponderadores de las entradas, los w, que se inicializan con valores aleatorios.\n",
    "\t1) Se deriva el costo con respecto a la función de activación ó predicción.\n",
    "\t2) Se deriva la predicción con respecto a la función de agregación.\n",
    "\t3) Se deriva la función de agregación, con respecto a los parámetros w.\n",
    "\t4) Se actualizan los parámetros.\n",
    "\n",
    "* Para derivar la composición de funciones J(Y(f(X))) se utiliza la denominada <b>regla de la cadena</b>.\n",
    "* Así, se propaga esa derivada hacia atrás, hasta llegar a los parámetros w iniciales, pasando capa tras capa por toda la red.</b>.<br>\n",
    "<img src=\"../_src/assets/redes_neuronales23.jpg\" height=\"300\"><br>\n",
    "\n",
    "* Entonces, con los datos de entrenamiento, la red ajusta sus parámetros, para adaptarse a la salida esperada.\n",
    "* Se llama <b>epoch ó ciclo</b>, al proceso completo desde un input de entrada, hasta la consecución de la función de coste, y posterior aplicación del algoritmo de Backpropagation de todos los ejemplos de entrenamiento.\n",
    "* El <b>tamaño de lote ó batch size</b> es el número de ejemplos de entrenamiento en un ciclo. Cuanto mayor sea el tamaño del lote, más espacio de memoria se necesitará.<br>\n",
    "<img src=\"../_src/assets/redes_neuronales24.jpg\" height=\"500\"><br>\n",
    "\n",
    "Enlace recomendado:\n",
    "* [Un lego a la vez] (https://medium.com/latinxinai/un-lego-a-la-vez-explicando-la-matem%C3%A1tica-de-como-las-redes-neuronales-aprenden-ae582ab91da6)\n",
    "* [Backpropagation] (https://www.youtube.com/watch?v=eNIqz_noix8)\n",
    "* [Las Matemáticas de Backpropagation] (https://www.youtube.com/watch?v=M5QHwkkHgAA)\n",
    "* [Aprendizaje profundo] (https://www.youtube.com/watch?v=aircAruvnKk)\n",
    "* [Descenso de Gradiente] (https://www.youtube.com/watch?v=IHZwWFHWa-w)\n",
    "* [Playground Tensorflow] (https://playground.tensorflow.org)\n",
    "\n",
    "## Función de Costo – Entropía Cruzada\n",
    "\n",
    "* Define una función de pérdida entre una etiqueta (y) y la probabilidad de pertenecer o no a esa etiqueta.\n",
    "* Pérdida para una instancia, caso binario: etiquetas y = 0 y 1.<br>\n",
    "<img src=\"../_src/assets/redes_neuronales25.jpg\" height=\"200\"><br>\n",
    "* Pérdida para una instancia<br>\n",
    "<img src=\"../_src/assets/redes_neuronales26.jpg\" height=\"50\"><br>\n",
    "* Costo para todas las instancias<br>\n",
    "<img src=\"../_src/assets/redes_neuronales27.jpg\" height=\"50\"><br>\n",
    "* Costo para todas las instancias, caso 1D<br>\n",
    "<img src=\"../_src/assets/redes_neuronales28.jpg\" height=\"50\"><br>\n",
    "\n",
    "## Funciones de Activación\n",
    "\n",
    "* Sigmoide / Logística:<br>\n",
    "<img src=\"../_src/assets/redes_neuronales29.jpg\" height=\"200\"><br>\n",
    "* Identidad: f(x) = x<br>\n",
    "<img src=\"../_src/assets/redes_neuronales30.jpg\" height=\"200\"><br>\n",
    "* Escalón: f(x) = 0 si x; 1 si x>=0<br>\n",
    "<img src=\"../_src/assets/redes_neuronales31.jpg\" height=\"200\"><br>\n",
    "* Tangente Hiperbólica: f(x)=tanh(x)<0<br>\n",
    "<img src=\"../_src/assets/redes_neuronales32.jpg\" height=\"200\"><br>\n",
    "* ReLU (Rectified Linear Units):   f(x) = 0 si x<0; x si x>=0\n",
    "  * En problemas de clasificación, lo más común es encontrar ReLU en las capas interiores y Sigmoide en la salida.<br>\n",
    "<img src=\"../_src/assets/redes_neuronales33.jpg\" height=\"250\"><br>\n",
    "\n",
    "## Perceptrón\n",
    "\n",
    "* Esta unidad neural, cuyo modelo se diseñó en 1958, se denominó Perceptrón.\n",
    "* No era sólo una función lineal, debido a que contaba con una función de activación.\n",
    "* La función de activación, devuelve probabilidades, que están entre 0 y 1.<br>\n",
    "\n",
    "<img src=\"../_src/assets/perceptron_multicapa1.jpg\" height=\"200\"><br>\n",
    "\n",
    "## Perceptrón Multicapa\n",
    "\n",
    "* La limitación con el perceptrón, es que sólo encuentra fronteras lineales, y como hemos visto, ampliar la red resuelve ese problema.\n",
    "* Multiclase: La cantidad de neuronas en la capa de salida tiene que ser igual a la cantidad de clases buscadas.<br>\n",
    "\n",
    "<img src=\"../_src/assets/perceptron_multicapa2.jpg\" height=\"200\"><br>\n",
    "\n",
    "## Regularización\n",
    "\n",
    "El problema fundamental de es la tensión existente entre optimización y generalización, donde <b>la optimización se considera respecto al proceso de ajustar un modelo para conseguir el mejor rendimiento posible sobre los datos de entrenamiento</b> (es donde se concentra la parte de aprendizaje en el Aprendizaje Automático), y <b>la generalización es respecto a lo bien que el modelo entrenado se comporta sobre datos que no ha visto anteriormente</b>. \n",
    "El objetivo es conseguir una buena generalización, pero es precisamente la parte que no podemos controlar, ya que solo podemos ajustar el modelo en función los datos de entrenamiento. Lo ideal sería poder conseguir una mayor, y lo más variada posible, cantidad de datos de entrenamiento, así, como cualquier proceso de aprendizaje, automático o natural: un modelo entrenado con más datos tendrá más herramientas para extrapolar su aprendizaje a situaciones nuevas.\n",
    "Como no siempre se pueden conseguir más datos, lo que hacemos es limitar la capacidad de aprendizaje del modelo, para prevenir el sobreajuste.\n",
    "El conjunto de técnicas utilizadas para tal fin, se conoce como Regularización.\n",
    "\n",
    "### Data Augmentation\n",
    "\n",
    "Cuando trabajamos con imágenes, hay algunas formas de incrementar el tamaño del conjunto de entrenamiento por medio de operaciones básicas: traslación, rotación, escalado, volteado, etc.<br>\n",
    "\n",
    "<img src=\"../_src/assets/regularizacion.jpg\" height=\"200\"><br>\n",
    "\n",
    "### Parada temprana (early stopping)\n",
    "\n",
    "Estrategia basada en validación cruzada, cuando observamos que el rendimiento en validación comienza a empeorar, paramos el entrenamiento del modelo.\n",
    "\n",
    "### L1 (Lasso)\n",
    "\n",
    "Coste es proporcional al valor absoluto de los pesos (matemáticamente, la norma L1 de los pesos). Suele dar como resultado que muchos pesos tomen el valor 0, por lo que a veces se identifica con un procedimiento de compresión de la red neuronal.\n",
    "\n",
    "### L2 (Ridge)\n",
    "\n",
    "Coste es proporcional al cuadrado de los valores de los pesos (matemáticamente, la norma L2 de los pesos). En el contexto de las redes neuronales, a esta norma también se le llama weight decay, ya que fuerza a que los pesos tiendan a 0.\n",
    "\n",
    "### Dropout\n",
    "\n",
    "Funciona como una capa que “apaga” neuronas de la capa anterior al azar. Al hacerlo, obliga a que ninguna se aprenda “de memoria” una muestra, sino que tengan que aprender entre todas.<br>\n",
    "\n",
    "<img src=\"../_src/assets/regularizacion2.jpg\" height=\"200\"><br>\n",
    "\n",
    "## Redes Neuronales - Librerías\n",
    "\n",
    "Con Python es posible programar desde cero y diseñar por completo una arquitectura de red neuronal. Sin embargo, también es posible implementar librerías que ya traen resuelto gran parte del código ocultando diferentes niveles de detalle dicha arquitectura.<br>\n",
    "\n",
    "<img src=\"../_src/assets/redes_neuronales34.jpg\" height=\"200\"><br>\n",
    "\n",
    "## Redes Neuronales Recurrentes\n",
    "\n",
    "Las Redes Neuronales Recurrentes ó RNN, además de tener conexiones con la capa anterior, las tienen con la misma capa. Con lo que combina la memoria de cada neurona con el input de la capa anterior al retroalimentarse. <br>\n",
    "\n",
    "<img src=\"../_src/assets/redes_recurrentes1.jpg\" height=\"200\"><br>\n",
    "\n",
    "* Procesa Secuencias de datos.\n",
    "* Cada elemento está correlacionado con el siguiente.\n",
    "* El tamaño de los datos de entrada es variable. <br>\n",
    "\n",
    "<img src=\"../_src/assets/redes_recurrentes2.jpg\" height=\"200\"><br>\n",
    "\n",
    "Hay numerosas variantes de asombrosa complejidad: <b>LSTM</b> se utilizan para reconocimiento de voz, texto manuscrito y patrones.\n",
    "\n",
    "* Agrega celda de estado.\n",
    "* Compuertas “forget” y “update”\n",
    "* Compuerta de salida<br>\n",
    "\n",
    "<img src=\"../_src/assets/redes_recurrentes3.jpg\" height=\"200\"><br>\n",
    "\n",
    "Enlaces sugeridos:\n",
    "* https://experiments.withgoogle.com/ai/ai-duet/view/\n",
    "* [Redes Transformers] (https://www.youtube.com/watch?v=Wp8NocXW_C4&feature=youtu.be)\n",
    "\n",
    "## Redes Neuronales Convolucionales\n",
    "\n",
    "* La Red Neuronal Convolucional ó CNN es el tipo de red neuronal más utilizada para el reconocimiento y procesamiento de imágenes.\n",
    "* Inspiradas en los procesos biológicos, en los que el patrón de conectividad entre neuronas se asemeja a la organización de la corteza visual de los seres vivos.\n",
    "* Son una variante del Perceptrón Multicapa y usan relativamente poco preprocesamiento en comparación con otros algoritmos de clasificación de imágenes. \n",
    "* Procesa la información mediante filtros y tiene la capacidad de calcular automáticamente los pesos de los filtros.\n",
    "* La forma en que se guarda una imagen, consiste básicamente en una matríz de valores que van de 0 a 255, e indican el valor del color para cada posición en el plano 2D, además están los canales, correspondientes a los valores RGB, así que esta matriz tiene 3 dimensiones.\n",
    "* Al analizar esa información, lo que queda como entrada para una red neuronal, es el array respectivo.<br>\n",
    "\n",
    "<img src=\"../_src/assets/redes_convolucionales1.jpg\" height=\"400\"><br>\n",
    "\n",
    "* En lugar de las capas completamente conectadas, la CNN aplica lo que se denomina capa convolucional, su funcionamiento es mucho más performante y de naturaleza muy diferente.\n",
    "* Su principal característica está en reconocer los bordes de la imagen, tanto verticales como horizontales.<br>\n",
    "\n",
    "<img src=\"../_src/assets/redes_convolucionales2.jpg\" height=\"200\"><br>\n",
    "\n",
    "¿Cómo funciona? Tenemos el ejemplo de una matriz de 6x6 a la cual, se le aplica una operación de convolución, que consistirá en un filtro, otra matriz más chica, en este caso de 3x3. El resultado será una matriz de 4x4:<br>\n",
    "\n",
    "<img src=\"../_src/assets/redes_convolucionales3.jpg\" height=\"200\"><br>\n",
    "<img src=\"../_src/assets/redes_convolucionales4.jpg\" height=\"200\"><br>\n",
    "<img src=\"../_src/assets/redes_convolucionales5.jpg\" height=\"200\"><br>\n",
    "<img src=\"../_src/assets/redes_convolucionales6.jpg\" height=\"200\"><br>\n",
    "\n",
    "* Si en la matriz original, hay sectores donde es muy marcada la diferencia numérica de los pixeles, significa que son zonas diferenciadas de la imagen.\n",
    "* De esta forma queda claro el sentido de la transición de claro a oscuro y viceversa:<br>\n",
    "\n",
    "<img src=\"../_src/assets/redes_convolucionales7.jpg\" height=\"200\"><br>\n",
    "<img src=\"../_src/assets/redes_convolucionales8.jpg\" height=\"200\"><br>\n",
    "\n",
    "* Los filtros pueden aplicarse también de manera horizontal, el resultado siempre va a ser una matriz donde queden identificados los limites en la imagen, es decir los bordes:<br>\n",
    "\n",
    "<img src=\"../_src/assets/redes_convolucionales9.jpg\" height=\"200\"><br>\n",
    "\n",
    "* Surgen dos inconvenientes, la imagen se hace más pequeña y además, hay pixeles que son repasados menos veces por el filtro, aquellos ubicados en los bordes. \n",
    "* Para resolverlo, se agranda la matriz agregando 1 o 2 pixeles más, quedando entonces para el ejemplo, una matriz de 8x8.\n",
    "* Por otra parte, la matriz de filtros, usualmente es impar, es decir, 3x3, 5x5 o 7x7.\n",
    "* Una variante es cambiar el <b>salto ó strided convolution</b> a 2 en lugar de 1.\n",
    "* Si la imagen es RGB, entonces en realidad tendremos 3 canales, así que el proceso de convolución es además sobre una tercer dimensión:<br>\n",
    "\n",
    "<img src=\"../_src/assets/redes_convolucionales10.jpg\" height=\"250\"><br>\n",
    "\n",
    "* Para crear una capa de la red neuronal convolucional, se aplica un <b>sesgo</b> y una función de activación <b>ReLU</b>:<br>\n",
    "\n",
    "<img src=\"../_src/assets/redes_convolucionales11.jpg\" height=\"250\"><br>\n",
    "\n",
    "* La capa de <b>agrupación máxima ó max pooling</b> es un nuevo tipo de filtro, que se queda con el valor máximo.\n",
    "* A veces, esta capa cuenta junto con la capa de convolución como una única capa, debido a que solo tiene hiperparámetros,  y no tiene parámetros que ajustar, o sea que no se practica propagación hacia atrás.<br>\n",
    "\n",
    "<img src=\"../_src/assets/redes_convolucionales12.jpg\" height=\"250\"><br>\n",
    "\n",
    "En una CNN entonces, se encuentran los 3 tipos de capas:\n",
    "1) Convolucional (CONV)\n",
    "2) Pool para agrupar (Pool)\n",
    "3) Totalmente conectada (FC: Fully conected)\n",
    "\n",
    "A medida que la red neuronal se hace más profunda, disminuye el tamaño de la matriz original, aumentando el número de canales:<br>\n",
    "\n",
    "<img src=\"../_src/assets/redes_convolucionales13.jpg\" height=\"250\"><br>\n",
    "\n",
    "AlexNet es una red con 60 millones de parámetros.<br>\n",
    "\n",
    "<img src=\"../_src/assets/redes_convolucionales14.jpg\" height=\"250\"><br>\n",
    "\n",
    "VGG-16 es un ejemplo de red con 138 millones de parámetros.<br>\n",
    "\n",
    "<img src=\"../_src/assets/redes_convolucionales15.jpg\" height=\"250\"><br>\n",
    "\n",
    "Neural Style Transfer\n",
    "\n",
    "<img src=\"../_src/assets/redes_convolucionales16.jpg\" height=\"250\"><br>\n",
    "\n",
    "\n",
    "\n",
    "## Homework"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d8f62de8d159179b6c7667a4d5d4ef6210275a6b2f6b808206464195fd50fc9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
